{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading dataset\n",
    "\n",
    "train = pd.read_csv('Data/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "#features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\n",
    "features = pd.read_csv('Data/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "#patient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "patient_notes = pd.read_csv('Data/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clean train dataset\n",
    "\n",
    "train_problem = pd.DataFrame(columns = ['id','case_num','pn_num','feature_num',\n",
    "                                        'annotation','location','raw_annotations','new_annotation',\n",
    "                                        'pn_history','feature_text','new_lens','right_annot','raw_new'])\n",
    "\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"dicts/frequency_dictionary_en.txt\"\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "def get_closest_word(word,sym_spell):\n",
    "    changes_flag = False\n",
    "    if len(word) > 1:\n",
    "        suggestions = sym_spell.lookup(\n",
    "            word, Verbosity.CLOSEST, max_edit_distance=2, transfer_casing=True\n",
    "        )\n",
    "        if len(suggestions) != 0:\n",
    "            if suggestions[0].distance > 0:\n",
    "                new_word = suggestions[0].term\n",
    "                changes_flag = True\n",
    "            else:\n",
    "                new_word = word\n",
    "        else:\n",
    "            new_word = word\n",
    "    else:\n",
    "        new_word = word\n",
    "    return new_word, changes_flag\n",
    "\n",
    "\n",
    "for ind, row in tqdm(train.iterrows()):\n",
    "    if len(row['annotation']) != 0:\n",
    "        changes_flag = False\n",
    "        new_annotations = []\n",
    "        new_lens = []\n",
    "        raw_annotations = []\n",
    "        raw_annotations_new = []\n",
    "        for index, (annotation, location) in enumerate(zip(row['annotation'],row['location'])):\n",
    "            #print((annotation, location))\n",
    "            new_annotation = []\n",
    "            composite = False\n",
    "            new_location_lens = []\n",
    "            raw_annotation =[]\n",
    "            raw_new = []\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                annot_by_loc = annotation[0:end-start]\n",
    "                annotation = annotation[end - start + 1:]\n",
    "                raw_annotation.append(annot_by_loc)\n",
    "                punctuation = re.findall(r\"[-,:/ \\.\\?\\!\\\"\\\";%0-9]+\", annot_by_loc)\n",
    "                words = re.findall(r\"[^-,:/ \\.\\?\\!\\\"\\\";%0-9]+\", annot_by_loc)\n",
    "                \n",
    "                if re.match(r\"[^-,:/ \\.\\?\\!\\\"\\\";%0-9]+\", annot_by_loc[0]) == None:\n",
    "                    words.insert(0,'')\n",
    "                if re.match(r\"[-,:/ \\.\\?\\!\\\"\\\";%0-9]+\", annot_by_loc[-1]) == None: \n",
    "                    punctuation.append('')\n",
    "\n",
    "                punctuation= iter(punctuation)\n",
    "                words = iter(words)\n",
    "                if composite:\n",
    "                    new_annotation.append(' ')\n",
    "                new_len = 0\n",
    "                new_phrase = ''\n",
    "                for word in words:\n",
    "                    new_word, changes_flag = get_closest_word(word,sym_spell)\n",
    "                    new_phrase += new_word\n",
    "                    punctuation_step = next(punctuation)\n",
    "                    new_phrase += punctuation_step\n",
    "                    new_len += len(new_word)+len(punctuation_step)\n",
    "                new_annotation.append(new_phrase)\n",
    "                raw_new.append(new_phrase)\n",
    "                new_location_lens.append(new_len)\n",
    "                composite = True\n",
    "            new_lens.append(new_location_lens)\n",
    "            new_annotation = ''.join(new_annotation)\n",
    "            new_annotations.append(new_annotation) \n",
    "            raw_annotations.append(raw_annotation)\n",
    "            raw_annotations_new.append(raw_new)\n",
    "        if changes_flag:\n",
    "            new_row = pd.DataFrame([[row['id'],row['case_num'],row['pn_num'],row['feature_num'],row['annotation'],\n",
    "                                     row['location'], raw_annotations, new_annotations,row['pn_history'],row['feature_text'],\n",
    "                                     new_lens, 0,raw_annotations_new]], columns = list(train_problem.columns))\n",
    "\n",
    "            train_problem = pd.concat([train_problem, new_row], ignore_index=True, axis = 0)\n",
    "\n",
    "            \n",
    "print(len(train_problem),' differences found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The way to check if typos has been found correctly \n",
    "#and fix them manually if needed\n",
    "\n",
    "train_problem = pd.read_pickle('typos/train_problem.pkl')\n",
    "problem_left = train_problem[train_problem['right_annot']==0]\n",
    "\n",
    "for ind, row in problem_left.iterrows():\n",
    "    print('Old annotation: ', row['annotation'])\n",
    "    print('Raw annotation: ', row['raw_annotations'])\n",
    "    print('New annotation: ', row['new_annotation'])\n",
    "    print(row['pn_history'])\n",
    "    right_annotation = eval(input('Which one is correct: '))\n",
    "    train_problem.loc[ind,'right_annot'] = right_annotation\n",
    "    clear_output()\n",
    "    train_problem.to_pickle('typos/train_problem.pkl')\n",
    "train_problem.to_pickle('typos/train_problem.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new lines\n",
    "\n",
    "extra_train = pd.read_pickle('typos/train_problem.pkl')\n",
    "extra_train = extra_train.drop(extra_train[extra_train.right_annot == 1 ].index)\n",
    "print('Rows to add: ', len(extra_train))\n",
    "extra_train['pn_history_new'] = np.zeros(len(extra_train))\n",
    "extra_train['new_location'] = np.zeros(len(extra_train)).astype(object)\n",
    "\n",
    "for ind, row in extra_train.iterrows():\n",
    "    if row['right_annot'] == 2:\n",
    "        raw_annot = row['raw_new']\n",
    "    else:\n",
    "        raw_annot = row['right_annot']\n",
    "        new_annotation = []\n",
    "        for an in raw_annot:\n",
    "            flag= False\n",
    "            n = ''\n",
    "            for ph in an:\n",
    "                if flag:\n",
    "                    n += ' ' + ph\n",
    "                else:\n",
    "                    n += ph\n",
    "                    flag = True\n",
    "            new_annotation.append(n)\n",
    "        extra_train.at[ind,'new_annotation'] = new_annotation   \n",
    "    pn_history_new = row['pn_history']\n",
    "    delay = np.array([[0,0]])\n",
    "    \n",
    "    \n",
    "    #change pn_history according to new annotation\n",
    "    for location, annot in zip(row['location'],raw_annot):\n",
    "        for loc, phrase in zip([s.split() for s in location.split(';')],annot): \n",
    "            start, end = int(loc[0]), int(loc[1])\n",
    "            current_delay = sum(delay[delay[:,0]<=start][:,1])\n",
    "            before_annot = pn_history_new[0:start+current_delay]\n",
    "            after_annot = pn_history_new[end+current_delay:]\n",
    "            pn_history_new = before_annot + phrase + after_annot\n",
    "            delay = np.append(delay,[[end,len(phrase) - (end-start)]],axis =0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    #generate new location\n",
    "    new_location = []\n",
    "    for location, annot in zip(row['location'],raw_annot):\n",
    "        complicated = False\n",
    "        current_loc = ''\n",
    "        for loc, phrase in zip([s.split() for s in location.split(';')],annot): \n",
    "            start, end = int(loc[0]), int(loc[1])\n",
    "            current_delay = sum(delay[delay[:,0]<=start][:,1])\n",
    "            if complicated:\n",
    "                current_loc+= ';'+str(start+current_delay) +' ' + str(start+current_delay+len(phrase))\n",
    "            else:\n",
    "                current_loc+= str(start+current_delay) +' ' + str(start+current_delay+len(phrase))\n",
    "            complicated = True\n",
    "        new_location.append(current_loc)\n",
    "\n",
    "    extra_train.loc[ind,'pn_history_new'] = pn_history_new\n",
    "    extra_train.at[ind,'new_location'] = new_location\n",
    "    \n",
    "\n",
    "extra_train = extra_train.drop(columns=['annotation', 'location','raw_annotations','new_lens','right_annot','raw_new','pn_history'])\n",
    "extra_train = extra_train.rename(columns={'new_annotation': 'annotation','pn_history_new':'pn_history','new_location':'location'})\n",
    "extra_train = extra_train[['id','case_num','pn_num','feature_num','annotation','location','feature_text','pn_history']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the lines we created are correct: annotations are qual to phrase by location \n",
    "\n",
    "count = 0\n",
    "for ind, row in tqdm(extra_train.iterrows()):\n",
    "    for location, annot in zip(row['location'],row['annotation']):\n",
    "        flag = False\n",
    "        annot_holder =''\n",
    "        for loc in [s.split() for s in location.split(';')]:\n",
    "            start, end = int(loc[0]), int(loc[1])\n",
    "            fragment = row['pn_history'][start:end]\n",
    "            if flag:\n",
    "                annot_holder += ' '+ fragment\n",
    "            else:\n",
    "                annot_holder = fragment\n",
    "            flag = True\n",
    "        if annot != annot_holder:\n",
    "            print(annot,annot_holder)\n",
    "            print(row['pn_history'])\n",
    "            display(row)\n",
    "            count+=1\n",
    "print(count)\n",
    "if count == 0:\n",
    "    extra_train.to_pickle('typos/extra_train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
